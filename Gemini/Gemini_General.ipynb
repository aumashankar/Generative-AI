{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: General Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers:\n",
        "\n",
        "- Temperature\n",
        "- Max output length\n",
        "- Token counting\n"
      ],
      "metadata": {
        "id": "QOKxmDLtRogx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTEz0qtFvxC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAK8IsGF4Os"
      },
      "source": [
        "Since we've put our gemini API key in Colab Secrets, we can just run the following cells to setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38686c19-dd74-48b2-c5db-6d227c217968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Model Temperature\n",
        "\n",
        "Steve is indecisive about how to spend his Friday night as a Freshman at Berkeley. He decides to ask Gemini using the 1.5 Flash [variant](https://ai.google.dev/gemini-api/docs/models/gemini):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2bcfnGEviwTI",
        "outputId": "e8bf7b9e-f1b0-4e12-fab0-9af4c7575b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grab some friends and head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by competitive debugging. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwOhbdVkD_c"
      },
      "source": [
        "Steve is very smart and knows that Gemini is non-deterministic; the same prompt can result in different outputs! To demonstrate this, the following code passes the same prompt 5 different times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oxc8f6oQlCAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "d85aa488-73f5-4a5f-d016-2e13f9d4bc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Grab a late-night slice of pizza with your friends and brainstorm a creative solution to a challenging coding problem you're working on. \n",
            "\n",
            "2. Grab some friends, a few pizzas, and hack on a side project fueled by caffeine and late-night brainstorming at the always-open Soda Hall. \n",
            "\n",
            "3. Grab some fellow CS majors and head to a nearby arcade to test your coding skills (and reflexes) in a retro gaming showdown. \n",
            "\n",
            "4. Grab some fellow CS majors and head to the Berkeley AI Research Lab's (BAIR) weekly demo night for a dose of cutting-edge tech and stimulating conversation. \n",
            "\n",
            "5. Grab a few friends and code a game inspired by the latest Berkeley research paper while enjoying pizza at the legendary Cheeseboard Collective. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "outputs = []\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "for i in range(5):\n",
        "  response = model.generate_content(prompt)\n",
        "  outputs.append(response.text)\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA9ewE_dMFMZ"
      },
      "source": [
        "Gemini returns a different response each time. Hmm. Steve doesn't like the fact that his Friday night might be determined by random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFTcYStQ2YY"
      },
      "source": [
        "Fortunately, Gemini has a temperature parameter that controls the randomness of the output. Temperature values can range from 0.0 to 2.0. We can check the temperature of our current model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GPndrGi2nP5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "af1be0b3-56b7-4a15-d917-75e4da6d462b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == 'models/gemini-1.5-flash':\n",
        "        print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdqPso3kamW"
      },
      "source": [
        "Let's initialize a new model with a temperature of 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-GqupV-dYsvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "1f400d7f-7639-481a-d896-a974926447b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Grab some friends, head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by the thrill of solving a challenging LeetCode problem. \n",
            "\n",
            "2. Grab some friends, head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by the thrill of solving a challenging LeetCode problem. \n",
            "\n",
            "3. Grab some friends, head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by the thrill of solving a challenging LeetCode problem. \n",
            "\n",
            "4. Grab some friends, head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by the thrill of solving a challenging LeetCode problem. \n",
            "\n",
            "5. Grab some friends, head to the Soda Hall courtyard for a late-night coding session fueled by pizza and fueled by the thrill of solving a challenging LeetCode problem. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "new_outputs = []\n",
        "low_temp_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0})\n",
        "for i in range(5):\n",
        "  response = low_temp_model.generate_content(prompt)\n",
        "  new_outputs.append(response.text)\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs are the same! Finally Steve can be sure how to spend his night. Conversely, setting temperature to the max value of 2.0 would have the opposite effect, yielding more unpredictable responses."
      ],
      "metadata": {
        "id": "AZ78-JuN7XHi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5QDIG6cPbID"
      },
      "source": [
        "## Max Output Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MfrBmbPi5i"
      },
      "source": [
        "Let's say Steve wants his outputs to be below a certain length. In large language models, text is generated in tokens. For Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words. He can set the `max_output_tokens` variable as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16899e41-cc84-4045-f6a6-42a8b2cbd6aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit up a hackathon\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "short_response_model = genai.GenerativeModel(model_name, generation_config={\"max_output_tokens\": 5})\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = short_response_model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz31PRYSRMUt"
      },
      "source": [
        "Notice that this simply halts token generation at a fixed quantity and does not guarantee that the output is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4672af98ac57"
      },
      "source": [
        "## Token Count\n",
        "\n",
        "Let's say that Steve is being charged for every token that he inputs to Gemini.\n",
        "\n",
        "If Steve has billing enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important. As such, Steve might want to know the number of tokens in his prompt (input) before actually putting it into the model.\n",
        "\n",
        "Let's create a new instance of Gemini 1.5 Flash and set our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "338fb9a6af78"
      },
      "outputs": [],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "token_n_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.0})\n",
        "poem_prompt = \"Write me a poem about Berkeley's campus\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = token_n_model.generate_content(poem_prompt)\n",
        "response.text"
      ],
      "metadata": {
        "id": "9WVlpLcDLjOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "367c34de-cb15-4c13-fe3f-8630162574a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Where Sather Tower stands, a sentinel of stone,\\nAnd Campanile chimes, a melody unknown,\\nBerkeley's campus sprawls, a vibrant tapestry,\\nOf minds ablaze, and dreams that yearn to be.\\n\\nBeneath the oaks, where squirrels chase and play,\\nStudents gather, lost in thought all day,\\nDebating theories, pondering the unknown,\\nIn classrooms filled with wisdom, seeds are sown.\\n\\nThe scent of coffee, mingled with the breeze,\\nCarries whispers of revolution, if you please,\\nFor Berkeley's heart beats with a restless soul,\\nA spirit bold, that challenges the whole.\\n\\nFrom Sproul Plaza's hub, where voices rise,\\nTo Bancroft's bustle, where the bookstore lies,\\nA kaleidoscope of cultures, side by side,\\nA melting pot of passion, where dreams reside.\\n\\nThe Golden Bear, a symbol strong and proud,\\nWatches over students, in the bustling crowd,\\nA legacy of learning, passed down through the years,\\nA beacon of knowledge, dispelling doubts and fears.\\n\\nSo walk the paths, where history unfolds,\\nAnd let the spirit of Berkeley, take hold,\\nFor in this place, where minds are set aflame,\\nThe future's promise, whispers your name. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating a response, we can check how many tokens are in this prompt using the `.count_tokens` function of the model:"
      ],
      "metadata": {
        "id": "-1w3HL5nIyR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_count = token_n_model.count_tokens(poem_prompt)\n",
        "output_token_count = token_n_model.count_tokens(response.text)\n",
        "print(f'Tokens in prompt: {prompt_token_count} \\n Estimated tokens in output {output_token_count}')"
      ],
      "metadata": {
        "id": "lJ0V8H2dIf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e6eea09a-a4cc-47a3-fecf-498044a78a57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in prompt: total_tokens: 9\n",
            " \n",
            " Estimated tokens in output total_tokens: 271\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}